#!/usr/bin/evn python3
from JackTokenizer import JackTokenizer


class CompilationEngine:
    """
    CompilationEngine: Effects the actual compilation output. Gets its input from a
    JackTokenizer and emits its parsed structure into an output file/stream. The
    output is generated by a series of compilexxx() routines, one for every syntactic
    element xxx of the Jack grammar. The contract between these routines is that each
    compilexxx() routine should read the syntactic construct xxx from the input,
    advance() the tokenizer exactly beyond xxx, and output the parsing of xxx. Thus,
    compilexxx() may only be called if indeed xxx is the next syntactic element of the input.
    """

    def __init__(self, input_tokenizer, output_file):
        """
        Creates a new compilation engine with the given input and output.
        The next routine called must be compile_class().
        """
        # self.__tokenizer = input_tokenizer
        self.__tokenizer = input_tokenizer
        self.__stream = open(output_file, "w")
        self.__op = ['+', '-', '*', '/', '&amp;', '|', '&lt;', '&gt;', '=']
        self.__unary_op = ['-', '~']
        self.__key_word_constant = ['true', 'false', 'null', 'this']

    def close(self):
        """  closes file """
        self.__stream.close()

    @staticmethod
    def __check_name(name, name_to_check):
        if name != name_to_check:
            raise Exception("Expected " + name + " but got " + name_to_check)

    @staticmethod
    def __open_terminal(element):
        return "<" + element + ">"

    @staticmethod
    def __close_terminal(element):
        return "</" + element + ">"

    @staticmethod
    def __make_terminal(element, terminal):
        return CompilationEngine.__open_terminal(element) + " " + terminal + \
               " " + CompilationEngine.__close_terminal(element) + "\n"

    # className, varName, subroutineName: identifier
    def __compile_identifier_name(self):
        self.__tokenizer.advance()
        if self.__tokenizer.token_type() != JackTokenizer.IDENTIFIER:
            raise Exception("expected identifier")
        self.__stream.write(self.__make_terminal("identifier", self.__tokenizer.identifier()))

    # class: 'class' className '{' classVarDec* subroutineDec* '}'
    def compile_class(self):
        """ Compiles a complete class. """
        self.__tokenizer.advance()
        if self.__tokenizer.token_type() != JackTokenizer.KEYWORD:
            raise Exception("expected keyword class")
        self.__check_name("class", self.__tokenizer.key_word())
        self.__stream.write(self.__open_terminal("class") + "\n")
        self.__stream.write(self.__make_terminal("keyword", "class"))
        self.__compile_identifier_name()
        self.__compile_symbol("{")
        self.compile_class_var_dec()
        self.compile_subroutine()
        self.__compile_symbol("}")
        self.__stream.write(self.__close_terminal("class"))

    # type: 'int' | 'char' | 'boolean' | className
    # if void is True type also is 'void'
    def __compile_type(self, void=False):
        self.__tokenizer.advance()
        token_type = self.__tokenizer.token_type()
        if token_type == JackTokenizer.IDENTIFIER:
            self.__stream.write(self.__make_terminal("identifier", self.__tokenizer.identifier()))
        elif token_type == JackTokenizer.KEYWORD and self.__tokenizer.key_word() in ["int", "char", "boolean"]:
            self.__stream.write(self.__make_terminal("keyword", self.__tokenizer.key_word()))
        elif void and token_type == JackTokenizer.KEYWORD and self.__tokenizer.key_word() == "void":
            self.__stream.write(self.__make_terminal("keyword", self.__tokenizer.key_word()))
        else:
            raise Exception("expected 'type' of declared variables")

    # compile symbol, check if advanced tokenizer now has symbol type
    # and it is same as symbol_name and write to file
    def __compile_symbol(self, symbol_name):
        self.__tokenizer.advance()
        token_type = self.__tokenizer.token_type()
        if token_type != JackTokenizer.SYMBOL or self.__tokenizer.symbol() != symbol_name:
            res = "token type " + str(token_type) + " symbol: " + str(self.__tokenizer.symbol()) + " "
            raise Exception("expected symbol '" + symbol_name + "' " + res)
        self.__stream.write(self.__make_terminal("symbol", self.__tokenizer.symbol()))

    # compile (',' varName)*
    def __compile_more_var_names(self):
        self.__tokenizer.advance()
        if self.__tokenizer.token_type() != JackTokenizer.SYMBOL or self.__tokenizer.symbol() != ",":
            self.__tokenizer.back()
            return
        self.__stream.write(self.__make_terminal("symbol", self.__tokenizer.symbol()))
        self.__compile_identifier_name()
        self.__compile_more_var_names()

    # type varName (',' varName)* ';'
    def __compile_var_body(self):
        self.__compile_type()
        self.__compile_identifier_name()
        self.__compile_more_var_names()
        self.__compile_symbol(";")

    # classVarDec: ('static'|'field') type varName (',' varName)* ';'
    def compile_class_var_dec(self):
        """ Compiles a static declaration or a field declaration. """
        self.__tokenizer.advance()
        token_type = self.__tokenizer.token_type()
        if token_type != JackTokenizer.KEYWORD or \
                (self.__tokenizer.key_word() != "static" and self.__tokenizer.key_word() != "field"):
            self.__tokenizer.back()
            return
        self.__stream.write(self.__open_terminal("classVarDec") + "\n")
        self.__stream.write(self.__make_terminal("keyword", self.__tokenizer.key_word()))
        self.__compile_var_body()
        self.__stream.write(self.__close_terminal("classVarDec") + "\n")
        self.compile_class_var_dec()

    # subroutineDec: ('constructor'|'function'|'method')
    # ('void' | type) subroutineName '(' parameterList ')' subroutineBody
    def compile_subroutine(self):
        """ Compiles a complete method, function, or constructor. """
        self.__tokenizer.advance()
        token_type = self.__tokenizer.token_type()
        if token_type != self.__tokenizer.KEYWORD and self.__tokenizer.key_word() \
                not in ['constructor', 'function', 'method']:
            self.__tokenizer.back()
            return
        self.__stream.write(self.__open_terminal("subroutineDec") + "\n")
        self.__stream.write(self.__make_terminal("keyword", self.__tokenizer.key_word()))
        self.__compile_type(True)
        self.__compile_identifier_name()
        self.__compile_symbol("(")
        self.compile_parameter_list()
        self.__compile_symbol(")")
        self.__compile_subroutine_body()
        self.__stream.write(self.__close_terminal("subroutineDec") + "\n")
        self.compile_subroutine()

    # subroutineBody: '{' varDec* statements '}'
    def __compile_subroutine_body(self):
        self.__stream.write(self.__open_terminal("subroutineBody") + "\n")
        self.__compile_symbol("{")
        self.compile_var_dec()
        self.compile_statements()
        self.__compile_symbol("}")
        self.__stream.write(self.__close_terminal("subroutineBody") + "\n")

    # parameterList: ((type varName) (',' type varName)*)?
    def compile_parameter_list(self):
        """ Compiles a (possibly empty) parameter list, not including the enclosing ‘‘()’’. """
        self.__stream.write(self.__open_terminal("parameterList") + "\n")
        self.__tokenizer.advance()
        token_type = self.__tokenizer.token_type()
        if token_type == JackTokenizer.IDENTIFIER:
            self.__stream.write(self.__make_terminal("identifier", self.__tokenizer.identifier()))
        elif token_type == JackTokenizer.KEYWORD and self.__tokenizer.key_word() in ["int", "char", "boolean"]:
            self.__stream.write(self.__make_terminal("keyword", self.__tokenizer.key_word()))
        else:
            self.__stream.write(self.__close_terminal("parameterList") + "\n")
            self.__tokenizer.back()
            return
        self.__compile_identifier_name()
        self.__compile_parameter_more_args()
        self.__stream.write(self.__close_terminal("parameterList") + "\n")

    # (',' type varName)*
    def __compile_parameter_more_args(self):
        self.__tokenizer.advance()
        if self.__tokenizer.token_type() != JackTokenizer.SYMBOL or self.__tokenizer.symbol() != ",":
            self.__tokenizer.back()
            return
        self.__stream.write(self.__make_terminal("symbol", self.__tokenizer.symbol()))
        self.__compile_type()
        self.__compile_identifier_name()
        self.__compile_parameter_more_args()

    # varDec: 'var' type varName (',' varName)* ';'
    def compile_var_dec(self):
        """ Compiles a var declaration. """
        self.__tokenizer.advance()
        if self.__tokenizer.token_type() == JackTokenizer.KEYWORD and self.__tokenizer.key_word() != "var":
            self.__tokenizer.back()
            return
        self.__stream.write(self.__open_terminal("varDec") + "\n")
        self.__stream.write(self.__make_terminal("keyword", self.__tokenizer.key_word()))
        self.__compile_var_body()
        self.__stream.write(self.__close_terminal("varDec") + "\n")
        self.compile_var_dec()

    def compile_statements(self):
        """ Compiles a sequence of statements, not including the enclosing ‘‘{}’’."""
        self.__stream.write(self.__open_terminal("statements") + "\n")
        self.__compile_statement()
        self.__stream.write(self.__close_terminal("statements") + "\n")

    # statements: statement*
    def __compile_statement(self):
        self.__tokenizer.advance()
        token_type = self.__tokenizer.token_type()
        if token_type != self.__tokenizer.KEYWORD:
            self.__tokenizer.back()
            return
        if self.__tokenizer.key_word() == "let":
            self.compile_let()
        elif self.__tokenizer.key_word() == "if":
            self.compile_if()
        elif self.__tokenizer.key_word() == "while":
            self.compile_while()
        elif self.__tokenizer.key_word() == "do":
            self.compile_do()
        elif self.__tokenizer.key_word() == "return":
            self.compile_return()
        else:
            raise Exception("not valid statement")
        self.__compile_statement()

    # doStatement: 'do' subroutineCall ';'
    def compile_do(self):
        """ Compiles a do statement. """
        self.__stream.write(self.__open_terminal("doStatement") + "\n")
        self.__stream.write(self.__make_terminal("keyword", "do"))
        self.__compile_subroutine_call()
        self.__compile_symbol(";")
        self.__stream.write(self.__close_terminal("doStatement") + "\n")

    # subroutineCall: subroutineName '(' expressionList ')' |
    #                  (className | varName) '.' subroutineName '(' expressionList ')'
    def __compile_subroutine_call(self):
        self.__compile_identifier_name()
        self.__tokenizer.advance()
        token_type = self.__tokenizer.token_type()
        if token_type != JackTokenizer.SYMBOL:
            raise Exception("expected symbol in subroutine call")
        if self.__tokenizer.symbol() == ".":
            self.__stream.write(self.__make_terminal("symbol", "."))
            self.__compile_identifier_name()
        else:
            self.__tokenizer.back()
        self.__compile_expression_parameters()

    # compile '(' expressionList? ')'
    def __compile_expression_parameters(self):
        self.__compile_symbol("(")
        self.compile_expression_list()
        self.__compile_symbol(")")

    # letStatement: 'let' varName ('[' expression ']')? '=' expression ';'
    def compile_let(self):
        """ Compiles a let statement. """
        self.__stream.write(self.__open_terminal("letStatement") + "\n")
        self.__stream.write(self.__make_terminal("keyword", "let"))
        self.__compile_identifier_name()
        self.__tokenizer.advance()
        token_type = self.__tokenizer.token_type()
        if token_type == JackTokenizer.SYMBOL and self.__tokenizer.symbol() == "[":
            self.__stream.write(self.__make_terminal("symbol", "["))
            self.compile_expression()
            self.__compile_symbol("]")
        else:
            self.__tokenizer.back()
        self.__compile_symbol("=")
        self.compile_expression()
        self.__compile_symbol(";")
        self.__stream.write(self.__close_terminal("letStatement") + "\n")

    # whileStatement: 'while' '(' expression ')' '{' statements '}'
    def compile_while(self):
        """ Compiles a while statement. """
        self.__stream.write(self.__open_terminal("whileStatement") + "\n")
        self.__stream.write(self.__make_terminal("keyword", "while"))
        self.__compile_symbol("(")
        self.compile_expression()
        self.__compile_symbol(")")
        self.__compile_symbol("{")
        self.compile_statements()
        self.__compile_symbol("}")
        self.__stream.write(self.__close_terminal("whileStatement") + "\n")

    # ReturnStatement 'return' expression? ';'
    def compile_return(self):
        """ Compiles a return statement. """
        self.__stream.write(self.__open_terminal("returnStatement") + "\n")
        self.__stream.write(self.__make_terminal("keyword", "return"))
        self.__tokenizer.advance()
        if self.__tokenizer.token_type() != JackTokenizer.SYMBOL:
            self.__tokenizer.back()
            self.compile_expression()
        else:
            self.__tokenizer.back()
        self.__compile_symbol(";")
        self.__stream.write(self.__close_terminal("returnStatement") + "\n")

    # ifStatement: 'if' '(' expression ')' '{' statements '}'
    # ('else' '{' statements '}')?
    def compile_if(self):
        """ Compiles an if statement, possibly with a trailing else clause. """
        self.__stream.write(self.__open_terminal("ifStatement") + "\n")
        self.__stream.write(self.__make_terminal("keyword", "if"))
        self.__compile_symbol("(")
        self.compile_expression()
        self.__compile_symbol(")")
        self.__compile_symbol("{")
        self.compile_statements()
        self.__compile_symbol("}")
        self.__stream.write(self.__close_terminal("ifStatement") + "\n")
        # else changes to advance
        self.__tokenizer.advance()
        if self.__tokenizer.key_word() != JackTokenizer.KEYWORD or self.__tokenizer.key_word() != "else":
            self.__tokenizer.back()
            return
        self.__stream.write(self.__make_terminal("keyword", "else"))
        self.__compile_symbol("{")
        self.compile_statements()
        self.__compile_symbol("}")

    # expression: term (op term)*
    def compile_expression(self):
        """ Compiles an expression. """
        self.__stream.write(self.__open_terminal("expression") + "\n")
        self.compile_term()
        self.__compile_op_term()
        self.__stream.write(self.__close_terminal("expression") + "\n")

    # compile (op term)*
    def __compile_op_term(self):
        self.__tokenizer.advance()
        token_type = self.__tokenizer.token_type()
        if token_type != JackTokenizer.SYMBOL or self.__tokenizer.symbol() not in self.__op:
            self.__tokenizer.back()
            return
        self.__stream.write(self.__make_terminal("symbol", self.__tokenizer.symbol()))
        self.compile_term()
        self.__compile_op_term()

    # term: integerConstant | stringConstant | keywordConstant | varName |
    #       varName '[' expression ']' | subroutineCall | '(' expression ')' | unaryOp term
    def compile_term(self):
        """
        Compiles a term. This routine is faced with a slight difficulty
        when trying to decide between some of the alternative parsing
        rules. Specifically, if the current token is an identifier, the routine
        must distinguish between a variable, an array entry, and a
        subroutine call. A single look-ahead token, which may be one
        of ‘‘[’’, ‘‘(’’, or ‘‘.’’ suffices to distinguish between the three possibilities.
        Any other token is no part of this term and should not be advanced over.
        """
        self.__stream.write(self.__open_terminal("term") + "\n")
        self.__tokenizer.advance()
        token_type = self.__tokenizer.token_type()
        if token_type == JackTokenizer.INT_CONST:
            self.__stream.write(self.__make_terminal("integerConstant", self.__tokenizer.int_val()))
        elif token_type == JackTokenizer.STRING_CONST:
            self.__stream.write(self.__make_terminal("stringConstant", self.__tokenizer.string_val()))
        elif token_type == JackTokenizer.KEYWORD and self.__tokenizer.key_word() in self.__key_word_constant:
            self.__stream.write(self.__make_terminal("keyword", self.__tokenizer.key_word()))
        elif token_type == JackTokenizer.IDENTIFIER:
            self.__stream.write(self.__make_terminal("identifier", self.__tokenizer.identifier()))
            self.__tokenizer.advance()
            if self.__tokenizer.token_type() == JackTokenizer.SYMBOL:
                if self.__tokenizer.symbol() == ".":
                    self.__stream.write(self.__make_terminal("symbol", "."))
                    self.__compile_identifier_name()
                    self.__compile_expression_parameters()
                elif self.__tokenizer.symbol() == "(":
                    self.__stream.write(self.__make_terminal("symbol", "("))
                    self.__tokenizer.back()
                    self.__compile_expression_parameters()
                elif self.__tokenizer.symbol() == "[":
                    self.__stream.write(self.__make_terminal("symbol", "["))
                    self.compile_expression()
                    self.__compile_symbol("]")
                else:
                    self.__tokenizer.back()
            else:
                self.__tokenizer.back()
        elif token_type == JackTokenizer.SYMBOL:
            if self.__tokenizer.symbol() == "(":
                self.__stream.write(self.__make_terminal("symbol", "("))
                self.compile_expression()
                self.__compile_symbol(")")
            elif self.__tokenizer.symbol() in self.__unary_op:
                self.__stream.write(self.__make_terminal("symbol", self.__tokenizer.symbol()))
                self.compile_term()
        else:
            raise Exception("not valid command in term")
        self.__stream.write(self.__close_terminal("term") + "\n")

    # expressionList: expression (',' expression)*
    def compile_expression_list(self):
        """ Compiles a (possibly empty) comma-separated list of expressions. """
        self.__stream.write(self.__open_terminal("expressionList") + "\n")
        self.__tokenizer.advance()
        if self.__tokenizer.token_type() == JackTokenizer.SYMBOL and self.__tokenizer.symbol() == ")":
            self.__tokenizer.back()
            self.__stream.write(self.__close_terminal("expressionList") + "\n")
            return
        self.__tokenizer.back()
        self.compile_expression()
        self.__compile_more_expression_list()
        self.__stream.write(self.__close_terminal("expressionList") + "\n")

    # (',' expression)*
    def __compile_more_expression_list(self):
        self.__tokenizer.advance()
        if self.__tokenizer.token_type() != JackTokenizer.SYMBOL or self.__tokenizer.symbol() != ",":
            self.__tokenizer.back()
            return
        self.__stream.write(self.__make_terminal("symbol", ","))
        self.compile_expression()
        self.__compile_more_expression_list()
